{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHwTRpk0Ib0zWUNlR8buq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujayreddyv/CSUS-CSC219-MachineLeaning/blob/main/labs/CSC219Lab6_AutomaticHyperparameterTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSC219 Machine learning (Fall 2023)"
      ],
      "metadata": {
        "id": "wMvsQDIlmv3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lab6: Automatic hyperparameter tuning"
      ],
      "metadata": {
        "id": "i2InPdYA2NzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several libraries developed for tuning the hyperparameters of neural networks. One is the ***Keras Tuner*** for tuning Keras models.\n",
        "\n",
        "The Keras Tuner is somewhat similar to the Grid Search and Random Search in scikit-learn, and allows to define the search space for the hyperparameters over which the model will be fit, and it returns an optimal set of hyperparameters.\n",
        "\n",
        "Keras Tuner is not part of the `Keras` package and it needs to be installed and imported."
      ],
      "metadata": {
        "id": "HWAlWLV32bXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AJquzTl2EAW",
        "outputId": "786b7302-4870-4ae0-8f5c-77a82af8e638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "rkvDip-K2dyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc6870e-16c5-49c8-99a3-3308d57d9257"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Loading MNIST Dataset"
      ],
      "metadata": {
        "id": "dBe3swoF2N54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate tbe use of the Keras Tuner we will work with the MNIST dataset."
      ],
      "metadata": {
        "id": "_5iIMBTJ2ubH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(img_train, label_train), (img_test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "img_train = img_train.reshape(img_train.shape[0], 28, 28, 1)\n",
        "img_test = img_test.reshape(img_test.shape[0], 28, 28, 1)\n",
        "\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCuGY48N2umj",
        "outputId": "953381ab-6533-4e58-aa30-f586b142a801"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "\n",
        "# Converts a class vector (integers) to binary class matrix.   One-hot encoding!  Use with categorical_crossentropy.\n",
        "label_train = tf.keras.utils.to_categorical(label_train, num_classes)\n",
        "label_test = tf.keras.utils.to_categorical(label_test, num_classes)"
      ],
      "metadata": {
        "id": "rkbqGAF23AXx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Model Builder"
      ],
      "metadata": {
        "id": "Ze2EY4BK3BFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, a function called model_builder is created, which performs search over two hyperparameters:\n",
        "\n",
        "Number of neurons in the first Dense layer,\n",
        "Learning rate.\n",
        "The line hp_units = hp.Int('units', min_value=32, max_value=512, step=32) defines a grid search for the number of neurons in the Dense layer in the range [32, 64, 96, ..., 512].\n",
        "\n",
        "Next, a grid search for the learning rate is defined in the range [1e-2, 1e-3, 1e-4]."
      ],
      "metadata": {
        "id": "fwqOpCIi3Hnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 input_shape=(28, 28, 1)))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=tf.keras.losses.categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "koPeCPus3IS8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "s7p3Hi_K3M7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras Tuner has four tuning algorithms available:\n",
        "\n",
        "RandomSearch Tuner, similar to the Random Grid in scikit-learn performs a random search over a distribution of values for the hyperparameters.\n",
        "Hyperband Tuner, trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round, to converge to a high-performing model.\n",
        "BayesianOptimization Tuner, performs BayesianOptimization by creating a probabilistic mapping of the model to the loss function, and iteratively evaluating promising sets of hyperparameters.\n",
        "Sklearn Tuner, designed for use with scikit-learn models."
      ],
      "metadata": {
        "id": "9l0OebzG3STa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tuner = kt.Hyperband(model_builder,\n",
        "                 #    objective='val_accuracy',\n",
        "                 #    max_epochs=10,\n",
        "                 #    factor=3)\n",
        "\n",
        "#max_trials represents the number of hyperparameter combinations that will be tested by the tuner,\n",
        "#while execution_per_trial is the number of models that should be built and fit for each trial for robustness purposes.\n",
        "\n",
        "tuner = kt.BayesianOptimization(\n",
        "    model_builder,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=2,\n",
        "    executions_per_trial=1,\n",
        "    directory=\"mnist_kt_test\",\n",
        "    overwrite=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3MDWCftd3Wto"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code take about 1 min for each trial for the ENTIRE dataset"
      ],
      "metadata": {
        "id": "9yXV8VSx3ZZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(img_train[0:1000], label_train[0:1000], epochs=2, validation_data=(img_test[0:1000], label_test[0:1000]), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqixuDBg3YsL",
        "outputId": "62c1ff12-38c8-41dd-e82d-5c0aac672e66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 00m 08s]\n",
            "val_accuracy: 0.7260000109672546\n",
            "\n",
            "Best val_accuracy So Far: 0.753000020980835\n",
            "Total elapsed time: 00h 00m 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]          # num_trials: Optional number of HyperParameters objects to return.\n",
        "\n",
        "print(f\"Optimal number of neuron in the Dense layer: {best_hps.get('units')}\")\n",
        "print (f\"Optimal learning rate: {best_hps.get('learning_rate')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrPPv70I3f_R",
        "outputId": "d971e0bf-e75b-4b37-e6cb-d2b3c1468f2c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of neuron in the Dense layer: 64\n",
            "Optimal learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Train and Evaluate"
      ],
      "metadata": {
        "id": "VWR3MAdg3g2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will use the optimal hyperparameters from the Keras Tuner to create a model, and afterward we will evaluate the accuracy on the test dataset."
      ],
      "metadata": {
        "id": "X3Ph9Ot83jzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 2 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(img_train[0:1000], label_train[0:1000], epochs=2, validation_data=(img_test[0:1000], label_test[0:1000]), verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPRgqKjt3kCA",
        "outputId": "c2958020-2e50-4596-fb57-23261094f54b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "32/32 - 3s - loss: 1.1725 - accuracy: 0.6050 - val_loss: 0.7037 - val_accuracy: 0.7490 - 3s/epoch - 81ms/step\n",
            "Epoch 2/2\n",
            "32/32 - 2s - loss: 0.5706 - accuracy: 0.8230 - val_loss: 0.6417 - val_accuracy: 0.7650 - 2s/epoch - 77ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4b7ad11570>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(img_test[0:1000], label_test[0:1000])\n",
        "print(\"[val loss, val accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYq2YbVT3kUQ",
        "outputId": "c0a755c5-a6db-4cf8-fbb7-0b19d9eb0493"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6417 - accuracy: 0.7650\n",
            "[val loss, val accuracy]: [0.6416994333267212, 0.7649999856948853]\n"
          ]
        }
      ]
    }
  ]
}